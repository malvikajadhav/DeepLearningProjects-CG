{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "385c1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "import torch\n",
    "import torch.nn as nn  # All neural network modules, nn.Linear, nn.Conv2d, BatchNorm, Loss functions\n",
    "import torch.optim as optim  # For all Optimization algorithms, SGD, Adam, etc.\n",
    "import torchvision.transforms as transforms  # Transformations we can perform on our dataset\n",
    "import torchvision\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from skimage import io, color\n",
    "from torch.utils.data import (\n",
    "    Dataset,\n",
    "    DataLoader,\n",
    ")  # Gives easier dataset managment and creates mini batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ac0e0aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (4.6.0.66)\n",
      "Requirement already satisfied: numpy>=1.19.3 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from opencv-python) (1.22.3)\n",
      "Requirement already satisfied: scikit-image in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (0.19.3)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (1.4.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (2.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (2.8.8)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (8.4.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (1.7.3)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (2022.10.10)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from scikit-image) (1.22.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/akshaysharma/miniforge3/envs/tensorflow/lib/python3.9/site-packages (from packaging>=20.0->scikit-image) (3.0.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python\n",
    "!pip install scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b1414199",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_train_dir = os.path.join('./Dataset/face_images/face_images/train/face_images')\n",
    "img_test_dir = os.path.join('./Dataset/face_images/face_images/test/face_images')\n",
    "aug_train_dir = os.path.join('./Dataset/augment_images')\n",
    "lab_train_dir = os.path.join('./Dataset/lab_images')\n",
    "lab_test_dir = os.path.join('./Dataset/lab_images/test')\n",
    "l_train_dir = os.path.join('./Dataset/l_images')\n",
    "l_test_dir = os.path.join('./Dataset/l_images/test')\n",
    "a_train_dir = os.path.join('./Dataset/a_images/')\n",
    "a_test_dir = os.path.join('./Dataset/a_images/test')\n",
    "b_train_dir = os.path.join('./Dataset/b_images/')\n",
    "b_test_dir = os.path.join('./Dataset/b_images/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c6b37b",
   "metadata": {},
   "source": [
    "## Data Loading (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "777b5a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_a_train = glob.glob(os.path.join(a_train_dir,\"*.jpg\"))\n",
    "image_list_a_train.sort()\n",
    "image_list_b_train = glob.glob(os.path.join(b_train_dir,\"*.jpg\"))\n",
    "image_list_b_train.sort()\n",
    "input_image_list_train = glob.glob(os.path.join(l_train_dir,\"*.jpg\"))\n",
    "input_image_list_train.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46cddf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.annotations = input_image_list_train\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path =  input_image_list_train[index]\n",
    "        image = io.imread(img_path)\n",
    "        a_path = image_list_a_train[index]\n",
    "        a = io.imread(a_path)\n",
    "        b_path = image_list_b_train[index]\n",
    "        b = io.imread(b_path)\n",
    "        a = self.transform(a)\n",
    "        b = self.transform(b)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return (image, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a12f1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f154de12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-7\n",
    "batch_size = 10\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ca878888",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(TrainDataset(\n",
    "    csv_file=\"exp_input.csv\",\n",
    "    root_dir=\"CAP5404/l_images\",\n",
    "), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7707f",
   "metadata": {},
   "source": [
    "## Data Loading (Testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ccf9fe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list_a_test = glob.glob(os.path.join(a_test_dir,\"*.jpg\"))\n",
    "image_list_a_test.sort()\n",
    "image_list_b_test = glob.glob(os.path.join(b_test_dir,\"*.jpg\"))\n",
    "image_list_b_test.sort()\n",
    "input_image_list_test = glob.glob(os.path.join(l_test_dir,\"*.jpg\"))\n",
    "input_image_list_test.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "db898a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.annotations = input_image_list_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path =  input_image_list_test[index]\n",
    "        image = io.imread(img_path)\n",
    "        a_path = image_list_a_test[index]\n",
    "        a = io.imread(a_path)\n",
    "        b_path = image_list_b_test[index]\n",
    "        b = io.imread(b_path)\n",
    "        a = self.transform(a)\n",
    "        b = self.transform(b)\n",
    "        image = self.transform(image)\n",
    "\n",
    "        return (image, a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b52e531d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():  \n",
    "  dev = \"cuda:0\" \n",
    "else:  \n",
    "  dev = \"cpu\"  \n",
    "device = torch.device(dev) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fda8b7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 1e-7\n",
    "batch_size = 10\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7dca3048",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader( TestDataset(\n",
    "    csv_file=\"exp_input.csv\",\n",
    "    root_dir=\"CAP5404/l_images\",\n",
    "), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a3ef3",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6789c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(nn.Conv2d(1,3,kernel_size=2, padding=0,stride=2),\n",
    "                                 nn.ReLU(),\n",
    "                                nn.Conv2d(3,3,kernel_size=2, padding=0,stride=2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(3,3,kernel_size=2, padding=0,stride=2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(3,3,kernel_size=2, padding=0,stride=2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(3,3,kernel_size=2, padding=0,stride=2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(3,3,kernel_size=2, padding=0,stride=2),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Conv2d(3, 128, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(128),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(32),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(16),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(16, 9, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.BatchNorm2d(9),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2),\n",
    "                                nn.Conv2d(9, 2, kernel_size=3, stride=1, padding=1),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Upsample(scale_factor=2))\n",
    "    \n",
    "                                \n",
    "                                 \n",
    "    def forward(self, input):\n",
    "        output = self.cnn(input)\n",
    "        #         mean_chrominance = self.regressor(output)\n",
    "\n",
    "        return  output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "0f63b658",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (cnn): Sequential(\n",
       "    (0): Conv2d(1, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(3, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(3, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (5): ReLU()\n",
       "    (6): Conv2d(3, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (7): ReLU()\n",
       "    (8): Conv2d(3, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (9): ReLU()\n",
       "    (10): Conv2d(3, 3, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (11): ReLU()\n",
       "    (12): Conv2d(3, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (14): ReLU()\n",
       "    (15): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (16): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (18): ReLU()\n",
       "    (19): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (20): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (21): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (22): ReLU()\n",
       "    (23): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (24): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (26): ReLU()\n",
       "    (27): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (28): Conv2d(16, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): BatchNorm2d(9, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (30): ReLU()\n",
       "    (31): Upsample(scale_factor=2.0, mode=nearest)\n",
       "    (32): Conv2d(9, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU()\n",
       "    (34): Upsample(scale_factor=2.0, mode=nearest)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model\n",
    "model = CNN()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "32dec542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "47a66045",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hy/9gqqd8391hg491d50ng36n2c0000gn/T/ipykernel_42746/3441715908.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;31m#img_path =  input_image_list[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/hy/9gqqd8391hg491d50ng36n2c0000gn/T/ipykernel_42746/1162596927.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mb_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_list_b_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m                                (plugin, kind))\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/imageio/v2.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0mimopen_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"legacy_mode\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ri\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mimopen_args\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/imageio/core/imopen.py\u001b[0m in \u001b[0;36mimopen\u001b[0;34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                     \u001b[0mcandidate_plugin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplugin_class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                     \u001b[0;31m# not installed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/site-packages/imageio/config/plugins.py\u001b[0m in \u001b[0;36mplugin_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mclazz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/tensorflow/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train Network\n",
    "import torchvision.transforms as transforms\n",
    "trans = transforms.Compose([transforms.ToTensor()])\n",
    "Train_losses = []\n",
    "Test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx,(data, a , b) in enumerate(train_loader):\n",
    "        #img_path =  input_image_list[index] \n",
    "        data = data.to(device=device)\n",
    "        target_a = a.to(device=device)\n",
    "        target_b = b.to(device=device)\n",
    "        target = torch.cat((a, b), 1)\n",
    "        # forward\n",
    "        pred = model(data)\n",
    "        loss_val = loss(pred, target)\n",
    "\n",
    "        Train_losses.append(loss_val.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_val.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx,(data, a , b) in enumerate(test_loader):\n",
    "        #img_path =  input_image_list[index] \n",
    "        data = data.to(device=device)\n",
    "        target_a = a.to(device=device)\n",
    "        target_b = b.to(device=device)\n",
    "        target = torch.cat((a, b), 1)\n",
    "        # forward\n",
    "        pred = model(data)\n",
    "        \n",
    "        loss_val = loss(pred, target)\n",
    "\n",
    "        Test_losses.append(loss_val.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss_val.backward()\n",
    "\n",
    "        # gradient descent or adam step\n",
    "        optimizer.step()\n",
    "\n",
    "print(f\"Average Train MSE is {sum(Train_losses)/len(Train_losses)}\")\n",
    "print(f\"Average Test MSE is {sum(Test_losses)/len(Test_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "c4cc0fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch_idx,(data, a , b) in enumerate(test_loader):\n",
    "        #img_path =  input_image_list[index] \n",
    "        data = data.to(device=device)\n",
    "        target_a = a.to(device=device)\n",
    "        target_b = b.to(device=device)\n",
    "        # forward\n",
    "        pred = model(data)\n",
    "        save_predict_image(pred, data, batch_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "99d4b562",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(grayscale_input, ab_input, save_path=None, save_name=None):\n",
    "    '''Show/save rgb image from grayscale and ab channels\n",
    "    Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "    plt.clf() # clear matplotlib \n",
    "    color_image = torch.cat((grayscale_input, ab_input), 0).detach().numpy() # combine channels\n",
    "    color_image = color_image + 1\n",
    "    color_image = color_image - color_image.min()\n",
    "    color_image = color_image / (color_image.max() - color_image.min())\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = color.lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path + \"grayscale\", save_name), cmap='gray')\n",
    "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "6f782147",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predict_image(pred, data, batch_idx):\n",
    "    for i in range(len(data)):\n",
    "        name = str(batch_idx) + \" \" + str(i)\n",
    "        to_rgb(data[i], pred[i], save_path = \"./predicted_images/\", save_name = f\"{name}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "36eb5387",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./colorizer_250.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8899ef",
   "metadata": {},
   "source": [
    "# NCD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "7327b723",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncd_test = glob.glob(os.path.join(\"./Dataset/Gray/Cherry/\",\"*.jpg\"))\n",
    "ncd_test.sort()\n",
    "ncd_original = glob.glob(os.path.join(\"./Dataset/ColorfulOriginal/Cherry/\",\"*.jpg\"))\n",
    "ncd_original.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "758f03fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCDTestDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transforms.Compose([transforms.ToTensor()])\n",
    "        self.annotations = ncd_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img_path = ncd_test[index]\n",
    "        img_path_o = ncd_original[index]\n",
    "        T = transforms.Resize((128, 128))\n",
    "        image = Image.open(img_path)\n",
    "        image_o = Image.open(img_path_o)\n",
    "        image = T(image)\n",
    "        image_o = T(image_o)\n",
    "        image = self.transform(image)\n",
    "        image_o = self.transform(image_o)\n",
    "        return (image, image_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "ff497609",
   "metadata": {},
   "outputs": [],
   "source": [
    "ncd_test_loader = DataLoader(NCDTestDataset(\n",
    "    csv_file=\"exp_input.csv\",\n",
    "    root_dir=\"CAP5404/l_images\",\n",
    "), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "2d3586d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for b, (image, image_o) in enumerate(ncd_test_loader):\n",
    "    pred = model(image)\n",
    "    \n",
    "    save_predict_image_ncd(pred, image, b, image_o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "66ad419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "97c3fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb_ncd(grayscale_input, ab_input, original, save_path=None, save_name=None):\n",
    "    '''Show/save rgb image from grayscale and ab channels\n",
    "    Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "    plt.clf() # clear matplotlib \n",
    "    color_image = torch.cat((grayscale_input, ab_input), 0).detach().numpy() # combine channels\n",
    "    color_image = color_image + 1\n",
    "    color_image = color_image - color_image.min()\n",
    "    color_image = color_image / (color_image.max() - color_image.min())\n",
    "    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n",
    "    color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "    color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128\n",
    "    color_image = color.lab2rgb(color_image.astype(np.float64))\n",
    "    grayscale_input = grayscale_input.squeeze().numpy()\n",
    "    original = original.numpy()\n",
    "    original = original.transpose()\n",
    "\n",
    "    plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path + \"grayscale\", save_name), cmap='gray')\n",
    "    plt.imsave(arr=original, fname='{}{}'.format(save_path + \"original\", save_name))\n",
    "    plt.imsave(arr=color_image, fname='{}{}'.format(save_path, save_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "affdccc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predict_image_ncd(pred, data, batch_idx, image_o):\n",
    "    for i in range(len(data)):\n",
    "        name = str(batch_idx) + \" \" + str(i)\n",
    "        to_rgb_ncd(data[i], pred[i], image_o[i], save_path = \"./ncd_predicted_images/\", save_name = f\"{name}.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "c6d10884",
   "metadata": {},
   "outputs": [],
   "source": [
    "original = glob.glob(os.path.join(r\"./ncd_predicted_images/original*.jpg\"))\n",
    "predicted = glob.glob(os.path.join(r\"./ncd_predicted_images/[0,1,2]*.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "c2781798",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted.sort()\n",
    "original.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "d2824421",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ddcf9768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3580939012461708\n",
      "0.4464937619769425\n",
      "0.36875340394001227\n",
      "0.34434305279784244\n",
      "0.41021283981408335\n",
      "0.4150615420542478\n",
      "0.37169179272873526\n",
      "0.2727458441569837\n",
      "0.3770320595549275\n",
      "0.3308623104401629\n",
      "0.41748914096369893\n",
      "0.45059996720694223\n",
      "0.36869617798962323\n",
      "0.44936479363564086\n",
      "0.42191882162953426\n",
      "0.5533975959985756\n",
      "0.598348637153439\n",
      "0.5852125063175916\n",
      "0.5530422724326791\n",
      "0.3044505503521544\n",
      "0.2423542802433228\n",
      "0.31980157753880206\n",
      "0.3392061665853175\n",
      "0.40082590219422776\n",
      "0.40835275179971076\n",
      "0.3649599161375409\n",
      "0.3135458061661558\n",
      "0.3035956974943903\n",
      "0.4316711712778649\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/hy/9gqqd8391hg491d50ng36n2c0000gn/T/ipykernel_42746/2963163977.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moriginal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mssim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "for i in range(len(original)):\n",
    "    a = Image.open(original[i])\n",
    "    a = np.array(a)\n",
    "    b = Image.open(predicted[i])\n",
    "    b = np.array(b)\n",
    "    print(ssim(a, b, channel_axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412fcab1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
